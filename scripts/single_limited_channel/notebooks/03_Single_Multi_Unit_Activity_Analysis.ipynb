{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Unit and Multi-Unit Activity Analysis\n",
    "\n",
    "This notebook provides an end-to-end pipeline for analyzing single-unit and multi-unit activity (SUA/MUA) in electrophysiological data. The analysis involves loading data, preprocessing, spike sorting, feature extraction, clustering, spike train analysis, and visualization.\n",
    "\n",
    "### Objectives:\n",
    "- Isolate and analyze activity from single neurons.\n",
    "- Aggregate spikes from all recorded neurons for network activity analysis.\n",
    "- Perform spike-triggered averaging (STA) for sensory processing or motor control analysis.\n",
    "- Map receptive fields using spike-triggered correlation and decoding approaches.\n",
    "- Generate detailed neuronal response profiles.\n",
    "\n",
    "### Methods:\n",
    "- **Data Handling:** Load electrophysiological data using `Neo` and convert it to `SpikeInterface` format.\n",
    "- **Preprocessing:** Apply bandpass filtering, notch filtering, and common reference to the data.\n",
    "- **Spike Sorting:** Use advanced sorting algorithms like `Kilosort` with customizable parameters.\n",
    "- **Postprocessing and Feature Extraction:** Extract features from sorted spikes using PCA.\n",
    "- **Clustering:** Perform clustering using methods like `GMM`, `DBSCAN`, and `Agglomerative Clustering`.\n",
    "- **Spike Train Analysis:** Analyze spike trains for burst detection, synchrony, cross-correlation, and STA.\n",
    "- **Visualization:** Use `Matplotlib` and `Plotly` for static and interactive visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We start by importing the necessary Python libraries for data handling, preprocessing, spike sorting, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import neo  # For data handling\n",
    "import spikeinterface as si  # Core module for SpikeInterface\n",
    "import spikeinterface.extractors as se  # For data loading and extraction\n",
    "import spikeinterface.preprocessing as sp  # For data preprocessing\n",
    "import spikeinterface.sorters as ss  # For spike sorting algorithms\n",
    "import spikeinterface.postprocessing as spost  # For postprocessing sorted data\n",
    "import spikeinterface.qualitymetrics as sq  # For quality control metrics\n",
    "import elephant  # For advanced analysis on spike trains\n",
    "import elephant.statistics as es  # For statistical measures like firing rates\n",
    "import elephant.sta as esta  # For spike-triggered averaging\n",
    "import elephant.conversion as econv  # For converting spike trains\n",
    "import elephant.spike_train_correlation as escorr  # For correlation analysis\n",
    "import elephant.spectral as esp  # For spectral analysis\n",
    "import pyspike as ps  # For synchrony and burst detection\n",
    "import quantities as pq  # For unit handling\n",
    "import matplotlib.pyplot as plt  # For static visualization\n",
    "import plotly.express as px  # For interactive visualization\n",
    "import numpy as np  # For numerical operations\n",
    "from neo.io import NeuralynxIO, BlackrockIO, NixIO  # Example IO for Neo data loading\n",
    "from sklearn.decomposition import PCA  # For dimensionality reduction\n",
    "from sklearn.mixture import GaussianMixture  # For GMM clustering\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering  # For clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Handling\n",
    "\n",
    "We load the electrophysiological data using `Neo` and convert it to `SpikeInterface` format for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, io_type='NeuralynxIO'):\n",
    "    \"\"\"\n",
    "    Load electrophysiological data using Neo and convert to SpikeInterface format.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the file containing raw data.\n",
    "    - io_type (str): Type of Neo IO to use ('NeuralynxIO', 'BlackrockIO', 'NixIO', etc.).\n",
    "    \n",
    "    Returns:\n",
    "    - recording (si.BaseRecording): Loaded data in SpikeInterface's RecordingExtractor format.\n",
    "    \"\"\"\n",
    "    io_types = {\n",
    "        'NeuralynxIO': NeuralynxIO(dirname=file_path),\n",
    "        'BlackrockIO': BlackrockIO(filename=file_path),\n",
    "        'NixIO': NixIO(filename=file_path)\n",
    "    }\n",
    "    \n",
    "    if io_type not in io_types:\n",
    "        raise ValueError(f\"Unsupported IO type: {io_type}\")\n",
    "    \n",
    "    reader = io_types[io_type]\n",
    "    block = reader.read_block()\n",
    "    segment = block.segments[0]\n",
    "    analog_signal = segment.analogsignals[0]\n",
    "    recording = se.NeoRecordingExtractor(analog_signal)\n",
    "    return recording\n",
    "\n",
    "# Example Usage:\n",
    "example_file_path = 'data/sample_data'  # Adjust the path for your dataset\n",
    "recording = load_data(example_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "We preprocess the loaded data by applying bandpass filtering, optional notch filtering, and common referencing to remove noise and improve the quality of spike detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(recording, freq_min=300, freq_max=3000, notch_freq=None, common_ref_type='median'):\n",
    "    \"\"\"\n",
    "    Preprocess the loaded data by applying bandpass filtering, optional notch filtering, and common reference.\n",
    "    \n",
    "    Args:\n",
    "    - recording (si.BaseRecording): Loaded data in SpikeInterface's RecordingExtractor format.\n",
    "    - freq_min (int): Minimum frequency for bandpass filter.\n",
    "    - freq_max (int): Maximum frequency for bandpass filter.\n",
    "    - notch_freq (float): Frequency for notch filter to remove powerline noise. If None, skip.\n",
    "    - common_ref_type (str): Type of common reference ('median', 'average', etc.).\n",
    "    \n",
    "    Returns:\n",
    "    - recording_preprocessed (si.BaseRecording): Preprocessed data.\n",
    "    \"\"\"\n",
    "    # Apply bandpass filter\n",
    "    recording_bp = sp.bandpass_filter(recording, freq_min=freq_min, freq_max=freq_max)\n",
    "    \n",
    "    # Apply notch filter if specified\n",
    "    if notch_freq:\n",
    "        recording_notch = sp.notch_filter(recording_bp, freq=notch_freq)\n",
    "    else:\n",
    "        recording_notch = recording_bp\n",
    "    \n",
    "    # Apply common reference\n",
    "    recording_cmr = sp.common_reference(recording_notch, reference=common_ref_type)\n",
    "    \n",
    "    return recording_cmr\n",
    "\n",
    "# Example Usage:\n",
    "recording_preprocessed = preprocess_data(recording)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spike Sorting\n",
    "\n",
    "We perform spike sorting using advanced algorithms such as `Kilosort` to classify and isolate spikes from different neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_spikes(recording, sorter_name='kilosort2', custom_params=None):\n",
    "    \"\"\"\n",
    "    Perform spike sorting on the preprocessed data with configurable parameters.\n",
    "    \n",
    "    Args:\n",
    "    - recording (si.BaseRecording): Preprocessed recording data.\n",
    "    - sorter_name (str): Name of the sorting algorithm to use (e.g., 'kilosort2').\n",
    "    - custom_params (dict): Optional custom parameters for the sorting algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    - sorting (si.BaseSorting): Sorted spike data.\n",
    "    \"\"\"\n",
    "    sorter_params = custom_params if custom_params else ss.get_default_params(sorter_name)\n",
    "    sorting = ss.run_sorter(sorter_name, recording, output_folder='sorting_output', **sorter_params)\n",
    "    return sorting\n",
    "\n",
    "# Example Usage:\n",
    "sorting = sort_spikes(recording_preprocessed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Postprocessing and Feature Extraction\n",
    "\n",
    "We extract features from sorted spike waveforms to facilitate further analysis, such as clustering and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_sorting(sorting, recording):\n",
    "    \"\"\"\n",
    "    Extract features and waveforms from sorted spikes.\n",
    "    \n",
    "    Args:\n",
    "    - sorting (si.BaseSorting): Sorted spike data.\n",
    "    - recording (si.BaseRecording): Preprocessed recording data.\n",
    "    \n",
    "    Returns:\n",
    "    - waveform_extractor (si.WaveformExtractor): Extracted waveforms.\n",
    "    \"\"\"\n",
    "    waveform_extractor = spost.WaveformExtractor.create(recording, sorting, folder='waveforms', remove_existing_folder=True)\n",
    "    waveform_extractor.set_params(ms_before=1.5, ms_after=2.5)\n",
    "    waveform_extractor.run()\n",
    "    return waveform_extractor\n",
    "\n",
    "def extract_features(waveform_extractor, method='pca', n_components=3):\n",
    "    \"\"\"\n",
    "    Extract features from the sorted spike waveforms for clustering using PCA.\n",
    "    \n",
    "    Args:\n",
    "    - waveform_extractor (si.WaveformExtractor): Extracted waveforms.\n",
    "    - method (str): Method of feature extraction ('pca', 'waveform').\n",
    "    - n_components (int): Number of PCA components.\n",
    "    \n",
    "    Returns:\n",
    "    - features (np.ndarray): Feature matrix.\n",
    "    \"\"\"\n",
    "    waveforms = waveform_extractor.get_waveforms()\n",
    "    if method == 'pca':\n",
    "        pca = PCA(n_components=n_components)\n",
    "        features = pca.fit_transform(waveforms.reshape(waveforms.shape[0], -1))\n",
    "    else:\n",
    "        # Simple feature extraction: mean and std of waveforms\n",
    "        spike_width = np.mean(np.abs(waveforms), axis=(1, 2))\n",
    "        spike_amplitude = np.std(waveforms, axis=(1, 2))\n",
    "        features = np.column_stack((spike_width, spike_amplitude))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example Usage:\n",
    "waveform_extractor = postprocess_sorting(sorting, recording_preprocessed)\n",
    "features = extract_features(waveform_extractor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering and Spike Train Analysis\n",
    "\n",
    "We cluster the extracted spike features using advanced clustering algorithms and analyze spike trains for burst detection, synchrony, and other measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_spikes(features, method='gmm', **kwargs):\n",
    "    \"\"\"\n",
    "    Cluster spikes using specified clustering algorithm.\n",
    "    \n",
    "    Args:\n",
    "    - features (np.ndarray): Feature matrix for clustering.\n",
    "    - method (str): Clustering method ('gmm', 'dbscan', 'hdbscan', etc.).\n",
    "    - kwargs: Additional parameters for clustering methods.\n",
    "    \n",
    "    Returns:\n",
    "    - labels (np.ndarray): Cluster labels for each spike.\n",
    "    \"\"\"\n",
    "    if method == 'gmm':\n",
    "        gmm = GaussianMixture(n_components=kwargs.get('n_components', 3))\n",
    "        labels = gmm.fit_predict(features)\n",
    "    elif method == 'dbscan':\n",
    "        db = DBSCAN(eps=kwargs.get('eps', 0.5), min_samples=kwargs.get('min_samples', 5))\n",
    "        labels = db.fit_predict(features)\n",
    "    elif method == 'agglomerative':\n",
    "        agc = AgglomerativeClustering(n_clusters=kwargs.get('n_clusters', 3))\n",
    "        labels = agc.fit_predict(features)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported clustering method: {method}\")\n",
    "    return labels\n",
    "\n",
    "# Example Usage:\n",
    "labels = cluster_spikes(features, method='gmm')\n",
    "print(\"Cluster Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "Visualize the results using `Matplotlib` and `Plotly` to provide comprehensive insights into neuronal activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_features(features, labels):\n",
    "    \"\"\"\n",
    "    Plot the clustered features using PCA or other feature extraction method.\n",
    "    \n",
    "    Args:\n",
    "    - features (np.ndarray): Feature matrix.\n",
    "    - labels (np.ndarray): Cluster labels.\n",
    "    \"\"\"\n",
    "    fig = px.scatter(x=features[:, 0], y=features[:, 1], color=labels)\n",
    "    fig.update_layout(title='Spike Clustering', xaxis_title='Feature 1', yaxis_title='Feature 2')\n",
    "    fig.show()\n",
    "\n",
    "# Example Usage:\n",
    "plot_cluster_features(features, labels)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
