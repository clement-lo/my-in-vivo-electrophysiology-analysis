{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spike Sorting and Clustering for Multi-Electrode Array (MEA) Data\n",
    "This notebook demonstrates an advanced pipeline for spike sorting and clustering analysis of multi-electrode array (MEA) in vivo electrophysiology data. The analysis includes data handling, preprocessing, spike sorting, feature extraction, dimensionality reduction, clustering, and network-level analyses.\n",
    "\n",
    "## Objectives\n",
    "- **Spike Sorting**: Perform advanced spike sorting to classify spikes from densely packed electrodes.\n",
    "- **Clustering and Visualization**: Apply dimensionality reduction and clustering algorithms to visualize and validate spike sorting results.\n",
    "- **Spike Train and Connectivity Analysis**: Analyze spike train dynamics and compute network connectivity using advanced methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import neo  # For data handling\n",
    "import spikeinterface as si  # Core module for SpikeInterface\n",
    "import spikeinterface.extractors as se  # For data loading and extraction\n",
    "import spikeinterface.preprocessing as sp  # For data preprocessing\n",
    "import spikeinterface.sorters as ss  # For spike sorting algorithms\n",
    "import spikeinterface.postprocessing as spost  # For postprocessing sorted data\n",
    "import spikeinterface.qualitymetrics as sq  # For quality control metrics\n",
    "import elephant.spike_train_correlation as escorr  # For spike train correlations\n",
    "import elephant.connectivity as econn  # For network connectivity analysis\n",
    "import pyspike as ps  # For synchrony and burst detection\n",
    "import quantities as pq  # For unit handling\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For static visualization\n",
    "import plotly.express as px  # For interactive visualization\n",
    "from sklearn.decomposition import PCA  # For dimensionality reduction\n",
    "from sklearn.manifold import TSNE  # For t-SNE visualization\n",
    "from umap import UMAP  # For UMAP visualization\n",
    "from sklearn.cluster import AffinityPropagation, DBSCAN, AgglomerativeClustering  # For clustering algorithms\n",
    "import hdbscan  # For HDBSCAN clustering\n",
    "from neo.io import NeuralynxIO, BlackrockIO  # Example IOs for Neo data loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Handling\n",
    "Load the MEA data using `Neo` and convert it to the `SpikeInterface` format. Various types of `Neo` IO classes can be used depending on the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mea_data(file_path, io_type='NeuralynxIO'):\n",
    "    \"\"\"\n",
    "    Load MEA data using Neo and convert to SpikeInterface format.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the file containing raw data.\n",
    "    - io_type (str): Type of Neo IO to use ('NeuralynxIO', 'BlackrockIO', etc.).\n",
    "    \n",
    "    Returns:\n",
    "    - recording (si.BaseRecording): Loaded data in SpikeInterface's RecordingExtractor format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        io_types = {\n",
    "            'NeuralynxIO': NeuralynxIO(dirname=file_path),\n",
    "            'BlackrockIO': BlackrockIO(filename=file_path)\n",
    "        }\n",
    "        \n",
    "        if io_type not in io_types:\n",
    "            raise ValueError(f\"Unsupported IO type: {io_type}\")\n",
    "        \n",
    "        reader = io_types[io_type]\n",
    "        block = reader.read_block()\n",
    "        analog_signal = block.segments[0].analogsignals[0]\n",
    "        recording = se.NeoRecordingExtractor(analog_signal)\n",
    "        return recording\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MEA data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "file_path = 'data/sample_mea_data'  # Replace with your data file path\n",
    "recording = load_mea_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "Apply bandpass filtering, normalization, and optional noise reduction techniques like Common Average Referencing (CAR) or Independent Component Analysis (ICA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(recording, freq_min=300, freq_max=6000, noise_reduction='CAR'):\n",
    "    \"\"\"\n",
    "    Apply bandpass filter, normalization, and optional noise reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "    - recording (si.BaseRecording): Loaded data in SpikeInterface's RecordingExtractor format.\n",
    "    - freq_min (int): Minimum frequency for bandpass filter.\n",
    "    - freq_max (int): Maximum frequency for bandpass filter.\n",
    "    - noise_reduction (str): Noise reduction technique ('CAR', 'ICA', etc.).\n",
    "    \n",
    "    Returns:\n",
    "    - recording_preprocessed (si.BaseRecording): Preprocessed data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        recording_filtered = sp.bandpass_filter(recording, freq_min=freq_min, freq_max=freq_max)\n",
    "        \n",
    "        if noise_reduction == 'CAR':\n",
    "            recording_filtered = sp.common_reference(recording_filtered, reference='median')\n",
    "        elif noise_reduction == 'ICA':\n",
    "            recording_filtered = sp.ica(recording_filtered)\n",
    "        \n",
    "        recording_normalized = sp.zscore(recording_filtered)\n",
    "        return recording_normalized\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "recording_preprocessed = preprocess_data(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "Apply bandpass filtering, normalization, and optional noise reduction techniques like Common Average Referencing (CAR) or Independent Component Analysis (ICA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(recording, freq_min=300, freq_max=6000, noise_reduction='CAR'):\n",
    "    \"\"\"\n",
    "    Apply bandpass filter, normalization, and optional noise reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "    - recording (si.BaseRecording): Loaded data in SpikeInterface's RecordingExtractor format.\n",
    "    - freq_min (int): Minimum frequency for bandpass filter.\n",
    "    - freq_max (int): Maximum frequency for bandpass filter.\n",
    "    - noise_reduction (str): Noise reduction technique ('CAR', 'ICA', etc.).\n",
    "    \n",
    "    Returns:\n",
    "    - recording_preprocessed (si.BaseRecording): Preprocessed data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        recording_filtered = sp.bandpass_filter(recording, freq_min=freq_min, freq_max=freq_max)\n",
    "        \n",
    "        if noise_reduction == 'CAR':\n",
    "            recording_filtered = sp.common_reference(recording_filtered, reference='median')\n",
    "        elif noise_reduction == 'ICA':\n",
    "            recording_filtered = sp.ica(recording_filtered)\n",
    "        \n",
    "        recording_normalized = sp.zscore(recording_filtered)\n",
    "        return recording_normalized\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "recording_preprocessed = preprocess_data(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spike Sorting\n",
    "Perform spike sorting using advanced sorters like Kilosort, IronClust, and MountainSort. Custom parameters can be provided for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_spike_sorting(recording, sorter_name='kilosort2', custom_params=None):\n",
    "    \"\"\"\n",
    "    Perform spike sorting on MEA data using advanced sorters.\n",
    "    \n",
    "    Args:\n",
    "    - recording (si.BaseRecording): Preprocessed recording data.\n",
    "    - sorter_name (str): Name of the spike sorting algorithm (e.g., 'kilosort2').\n",
    "    - custom_params (dict): Optional custom parameters for the sorting algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    - sorting (si.BaseSorting): Sorted spike data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sorter_params = custom_params if custom_params else ss.get_default_params(sorter_name)\n",
    "        sorting = ss.run_sorter(sorter_name, recording, output_folder='sorting_output', **sorter_params)\n",
    "        return sorting\n",
    "    except Exception as e:\n",
    "        print(f\"Error in spike sorting: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "sorting = perform_spike_sorting(recording_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Postprocessing and Quality Metrics\n",
    "Extract waveforms and compute quality metrics like Signal-to-Noise Ratio (SNR), Inter-Spike Interval (ISI) violations, and firing rates for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_sorting(sorting, recording):\n",
    "    \"\"\"\n",
    "    Extract waveforms and compute quality metrics for sorted units.\n",
    "    \n",
    "    Args:\n",
    "    - sorting (si.BaseSorting): Sorted spike data.\n",
    "    - recording (si.BaseRecording): Preprocessed recording data.\n",
    "    \n",
    "    Returns:\n",
    "    - waveform_extractor (si.WaveformExtractor): Extracted waveforms.\n",
    "    - quality_metrics (dict): Quality metrics for each sorted unit.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        waveform_extractor = spost.WaveformExtractor.create(recording, sorting, folder='waveforms', remove_existing_folder=True)\n",
    "        waveform_extractor.set_params(ms_before=1.5, ms_after=2.5)\n",
    "        waveform_extractor.run()\n",
    "        quality_metrics = sq.compute_quality_metrics(waveform_extractor, metric_names=['snr', 'isi_violation', 'firing_rate'])\n",
    "        return waveform_extractor, quality_metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error in postprocessing sorting: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "waveform_extractor, quality_metrics = postprocess_sorting(sorting, recording_preprocessed)\n",
    "print(\"Quality Metrics:\", quality_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction and Dimensionality Reduction\n",
    "Apply feature extraction methods (e.g., PCA, t-SNE, UMAP) to reduce the dimensionality of the high-dimensional spike features for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_spikes(features, method='HDBSCAN', **kwargs):\n",
    "    \"\"\"\n",
    "    Cluster spikes using specified clustering algorithm.\n",
    "    \n",
    "    Args:\n",
    "    - features (np.ndarray): Feature matrix for clustering.\n",
    "    - method (str): Clustering method ('AffinityPropagation', 'HDBSCAN', 'DBSCAN', 'AgglomerativeClustering').\n",
    "    \n",
    "    Returns:\n",
    "    - labels (np.ndarray): Cluster labels for each spike.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if method == 'AffinityPropagation':\n",
    "            clustering = AffinityPropagation().fit(features)\n",
    "        elif method == 'HDBSCAN':\n",
    "            clustering = hdbscan.HDBSCAN().fit(features)\n",
    "        elif method == 'DBSCAN':\n",
    "            clustering = DBSCAN(eps=0.5, min_samples=5).fit(features)\n",
    "        elif method == 'AgglomerativeClustering':\n",
    "            clustering = AgglomerativeClustering(n_clusters=3).fit(features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported clustering method: {method}\")\n",
    "        return clustering.labels_\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clustering spikes: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "labels = cluster_spikes(reduced_features, method='HDBSCAN')\n",
    "print(\"Cluster Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering and Validation\n",
    "Cluster the reduced features using advanced clustering algorithms like Affinity Propagation, HDBSCAN, DBSCAN, or Agglomerative Clustering to validate sorted units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_spikes(features, method='HDBSCAN', **kwargs):\n",
    "    \"\"\"\n",
    "    Cluster spikes using specified clustering algorithm.\n",
    "    \n",
    "    Args:\n",
    "    - features (np.ndarray): Feature matrix for clustering.\n",
    "    - method (str): Clustering method ('AffinityPropagation', 'HDBSCAN', 'DBSCAN', 'AgglomerativeClustering').\n",
    "    \n",
    "    Returns:\n",
    "    - labels (np.ndarray): Cluster labels for each spike.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if method == 'AffinityPropagation':\n",
    "            clustering = AffinityPropagation().fit(features)\n",
    "        elif method == 'HDBSCAN':\n",
    "            clustering = hdbscan.HDBSCAN().fit(features)\n",
    "        elif method == 'DBSCAN':\n",
    "            clustering = DBSCAN(eps=0.5, min_samples=5).fit(features)\n",
    "        elif method == 'AgglomerativeClustering':\n",
    "            clustering = AgglomerativeClustering(n_clusters=3).fit(features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported clustering method: {method}\")\n",
    "        return clustering.labels_\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clustering spikes: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "labels = cluster_spikes(reduced_features, method='HDBSCAN')\n",
    "print(\"Cluster Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spike Train and Connectivity Analysis\n",
    "Analyze spike train correlations and compute network connectivity using methods like Granger causality and Directed Transfer Function (DTF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spike_train_correlations(sorting):\n",
    "    \"\"\"\n",
    "    Compute cross-correlograms for spike trains using Elephant.\n",
    "    \n",
    "    Args:\n",
    "    - sorting (si.BaseSorting): Sorted spike data.\n",
    "    \n",
    "    Returns:\n",
    "    - correlations (np.ndarray): Spike train correlation matrix.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spike_trains = [sorting.get_unit_spike_train(unit_id) for unit_id in sorting.unit_ids]\n",
    "        correlations = escorr.corrcoef(spike_trains)\n",
    "        return correlations\n",
    "    except Exception as e:\n",
    "        print(f\"Error in computing spike train correlations: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "correlations = compute_spike_train_correlations(sorting)\n",
    "print(\"Spike Train Correlation Matrix:\", correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization of Clustering Results\n",
    "Visualize the clustering results using interactive 3D plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(reduced_features, labels):\n",
    "    \"\"\"\n",
    "    Visualize clustering results in 3D.\n",
    "    \n",
    "    Args:\n",
    "    - reduced_features (np.ndarray): Reduced feature matrix.\n",
    "    - labels (np.ndarray): Cluster labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fig = px.scatter_3d(x=reduced_features[:, 0], y=reduced_features[:, 1], z=reduced_features[:, 2], color=labels)\n",
    "        fig.update_layout(title='3D Clustering Visualization', xaxis_title='Component 1', yaxis_title='Component 2', zaxis_title='Component 3')\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in plotting clusters: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "plot_clusters(reduced_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook provides an advanced pipeline for spike sorting and clustering analysis of MEA data. The analysis covers data handling, preprocessing, spike sorting, feature extraction, dimensionality reduction, clustering, and network-level analyses."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
